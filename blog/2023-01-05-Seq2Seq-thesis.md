<<<<<<< HEAD
---
layout: post
title: My first post!
categories: [blogging]
published: true
---

# My Markdown page

This is a Markdown page
=======


# Thesis Note
## Abstract
- 딥러닝 네트워크로 대규모 라벨링된 훈련데이터셋 학습문제를 해결했다.
- 하지만 딥러닝 네트워크는 시퀀스에서 시퀀스로 맵핑하는데 적용되지 못했다.
- 일반적인 end-to-end 시퀀스 학습방법으로 시퀀스 구조에 대한 최소한의 가정을 만들었다.
- 멀티레이어 LSTM으로 입력 시퀀스를 고정된 차원의 백터로 매핑하고 또다른 심층 LSTM으로 백터에서 타겟 시퀀스를 디코드한다.
- 영어 문장을 프랑스 문장으로 변역하는 작업에 적용하였고
- OOV문제의 패널티를 가지고도 BLEU 점수가 높았고, 긴 문장에서 나타나는 문제들도 완화되었다.
- 앞서 사용된 SMT 시스템에서 출력된 1,000개의 가설들을 다시 점수를 매겨보았다. 더 높은 점수가 나왔다.
- 입력 문장을 역순으로 뒤집어서 모델에 넣어주었더니 더 좋은 결과가 나왔다.

## Introduction
---
### 강력한 기능의 DNN
- DNNs 음성인식과 물체 인식에서 좋은 결과를 보였다. 
- 적당한 단계의 병렬 연산도 잘 수행하면서 더 높은 성능을 보였다.
- N으로 분류하는 DNN은 2개의 은닉층으로 N의 재곱의 크기를 갖는다.
- 신경망이 전통적인 통계모델에 관련이 있지만, 그들은 복잡한 연산을 배운다.
- 더욱이, 라벨된 훈련데이터셋이 신경망의 파라미터들을 구체화 시킬 만큼 충분한 양을 가질때마다 큰 DNN은 역전파에 의해 지도학습한다.
- 그러므로 큰 규모의 DNN이 좋은 결과를 갖도록 설정하는 파라미터들이 존재한다면, 지도된 역전파를 통해서 파라미터들을 찾고, 문제를 해결할 수 있다.
--- 
### DNN의 한계
- 그들이 매우 유연하고 강하지만, 입력과 출력이 고정된 차원의 백터로 인코딩 된다는 한계가 있다.
- 해결해야하는 많은 어려운 문제들이 추정하기 어려운 길이의 시퀀스데이터로 되어 있는데 이러한 문제에 적용하는데 어려움이 있다.
- 음성인식이나 기계번역등이 시퀀스 데이터 문제들이다.
- 이처럼 질의 응답 장치는 질문을 대표하는 단어들의 시퀀스들을 답변에 대응하는 단어들의 시퀀스로 보내주는 것처럼 보인다.
- 도메인에 의존적이지 않은 방법으로 시퀀스에서 시퀀스로 매핑하는 학습이 유용할 것이 명확하다.
- 입력과 출력이 고정되어있는 차원 요구하는 DNN은 시퀀스 데이터를 처리하는데 도전이 놓여있다.
--- 
## LSTM으로 입력차원 고정문제 해결
- 간단한 LSTM 구조를 적용하여 일반적인 seq2seq문제들을 해결할 수 있다.
- 하나의 LSTM으로 하나의 시간격마다 입력데이터를 읽어, 고정된 차원의 백터로 표현한다.
- 또 다른 LSTM은 고정된 백터에서 출력문장을 추출한다.
- 두 번째 LSTM은 본질적으로 입력문장에 조건부라는 것만 제외하고는 RNN 언어모델이다.
- LSTM의 장기 의존성 문제 데이터를 성공적으로 학습하는 능력으로 입력과 출력의 상당한 시간지연으로도 자연스럽게 선택하도록 한다.

## seq2seq 모델의 결과
- WMT14 영어-프랑스어 번역 작업에서, 34.81 BLEU 점수를 받았다. 5개 LSTM의 앙상블로 번역했으며, left-to-right beam-search decoder를 사용했다.
- 당시  신경망을 통한 번역에서 가장 우수한 점수였으며,
- 전통 통계 번역보다도 좀 더 좋은 점수를 받았다.
- 단어사전은 8만개의 단어를 가지고 있었다.
- 마지막으로 SMT 베이스라인에서 좋은 번역을 한 1,000개의 문장을 다시 번역해서 점수를 매겨보니 더 높은 점수인 36.5를 얻었다.
## 실험의 특이점
- 다른 연구자들이 비슷한 구조로 실험한 것과 다르게 이번 연구에서는 놀랍게도 LSTM이 긴 문장들에도 강했다.
- 이러한 이유는 입력 문장의 단어를 역순으로 바꿨다. 훈련데이터와 테스트데이터의 타겟 문장의 단어들은 변경하지 않았다. 
- 이렇게 해서, 최적화 문제를 더 쉽게 만드는 단기 의존성을 도입하였다.
- 이러한 결과로 SGD가 긴 문장에 문제가 없는 LSTM을 학습할 수 있었다.
- 단어의 순서를 바꾸는 간단한 방법이 이번 실험에 큰 기술적 공헌을 했다.

## LSTM으로 입력과 다른 차원의 출력 차원
- LSTM의 장점으로 다양한 길이의 입력 시퀀스를 고정된 백터 표현으로 매핑할 수 있다.
- 번역이 원본을 다른말로 바꾸어 표현하는 경향이 있다는 걸 고려하면, 
- 번역의 목적은 LSTM이 의미를 파악하는 문장표현을 찾도록 하는 것이다.
- 다른 의미의 문장들은 멀고, 비슷한 의미들을 가진 문장들이 서로 가까이 있다는 점을 이용한다.
- 질적 평가가 이러한 주장을 뒷받침한다. 
- 논문의 모델이 어순을 인식하고, 능동태와 수동태에 꽤 안정적이다.
--- 
## Model
### RNN 모델의 한계
- RNN은 시퀀스로 피드포워드 신경망의 자연스러운 일반 형태이다.
- 입력 시퀀스를 받으면 표준 RNN은 출력 시퀀스를 연산한다.
	- 입력 시퀀스의 단어를 하나 받아서 연산하고
	- 출력하며 이렇게 시퀀스 데이터 전체에 반복한다.
- RNN은 쉽게 시퀀스를 시퀀스 데이터로 매핑한다.
	- 이전의 출력과 입력을 나란하게 넣어서 시퀀스 데이터를 매핑한다.
- 그러나 RNN은 복잡하고 단조롭지 않은 관계의 다른 길이의 입력 시퀀스와 출력 시퀀스의 문제에 RNN을 적용하는데 명확하지 않았다.
- 일반 시퀀스 학습에 간단한 전략으로 하나의 RNN을 통해 입력 시퀀스를 고정된 크기의 백터로 매핑하는 것이다.
- 매핑된 백터를 또 다른 RNN을 통해 타겟 시퀀스를 출력한다.
- 이렇게 RNN으로 학습하는 것은 발생하는 장기 의존성 문제 때문에 어렵다.
### LSTM으로 모델을 개선
- 그러나 장기 의존성 문제를 어느정도 학습 가능한 LSTM으로 다시 설정한다면 개선될 것이라 생각했다.
- LSTM의 목표는 길이가 T인 입력 시퀀스 x와 길이가 T'인 출력 시퀀스 y의 조건부 확률 p를 평가하는 것이다.
- LSTM은 입력 시퀀스로부터 처음 얻어진 고정된 차원의 표현 v가 LSTM의 은닉층 마지막에 주어진다.
- 그리고 나서, 초기 은닉상태가 입력 시퀀스의 컨텍스트 벡터 v로  설정된 표준 LSTM-LM 조합을 이용해 y의 확률을 계산한다.
- 이 식을 통해서 각각의 확률 $p(y_{t} | v, v_1, \ldots, x_T )$의 분포가 단어장의 모든 단어의 소프트맥스로 표현된다.
- Graves의 LSTM 공식을 사용했고
- 모델이 모든 가능한 길이의 문장의 분포를 정의하도록 하기 위해 각각의 문장이 EOS로 끝난다.
- LSTM이 'A', 'B', 'C', '\<EOS>'의 표현(벡터)을 연산하여 그 표현을 사용해서 'W', 'X', 'Y', 'Z', '\<EOS>'의 확률을 연산한다.

### 실제 모델은 3가지가 다르다.
#### 입력과 출력에 다른 LSTM
- 하나는, 다른 종류의 LSTM 2개를 사용한다.
	- 하나는 입력 시퀀스를 위한 것이며
	- 하나는 출력 시퀀스를 위한 것이다.
- 이렇게 해서, 무시할만한 연산 비용으로 모델 파라미터들을 늘려서 LSTM이 다양한 언어 쌍을 동시에 학습하는 것을 자연스럽게 해준다.
#### 층이 깊은 LSTM
- 깊은 LSTM이 얉은 LSTM보다 좋은 성능을 보여줬다. 그래서 4개 층을 가진 LSTM을 사용한다.
#### 입력 시퀀스의 단어들을 뒤집어 성능 향상
- 입력 시퀀스의 단어들의 순서를 뒤집는 것이 매우 중요하다는 것을 팔견했다.

## 실험
- 이 논문은 새로운 방법을 WMT'14 영어에서 프랑스어로 기계번역하는 업무를 두 가지 방법으로 적용하였다.
- 레퍼런스 SMT 시스템을 사용하지 않고 입력 문장을 집적 번역하고 SMT 베이스라인에서 n-best 목록을 점수 다시 매겨 보았다.
- 이러한 번역 작업의 정확도를 보고하고, 샘플 번역들을 보여주고 결과 문장의 표현을 시각화한다.
### 데이터셋 상세
- 연구에서 WMT'14 영어-프랑스어 데이터셋을 사용했다.
- 우리가 모델을 3억4800만개 프랑스어 단어와 3억4백만개의 단어로 구성된 12,000,000개 문장의 깨끗하게 정돈된 부분집합으로 학습시켰다. 
- 공공의 접근성 때문에, 이러한 번역 작업과 특정한 훈련 데이터셋의 부분집합을 골랐다.
- 토큰화된 훈련, 테스트 셋이 베이스라인 SMT로부터 최고의 1,000개 목록과 함께 있기 때문이다.

- 전형적인 신경 언어 모델은 각 단어에 대한 벡터 표현에 의존하지만, 연구에서는 두 언어에 대한 고정된 단어를 사용한다.
- 소스 데이터에서 160,000개의 자주 쓰는 단어를 사용했고, 타겟 데이터에서 80,000개의 자주 쓰는는 단어를 사용했다.
- 모든 OOV 단어들은 특별한 'UNK' 토큰으로 대체했다.

### 디코딩과 재점수
- 이번 실험의 핵심은 크고 깊은 LSTM을 많은 문장쌍으로 학습시키는 것이다.
- 연구에서 입력된 소스 문장 S에 올바른 변역 T가 나올 Log확률을 최대화 하도록 학습시켰다.
$$\frac{1}{ |S|}\sum\limits_{(T,S) \in S}log  \ p(T|S)$$
- S는 훈련 데이터셋이다.
- 일단 훈련이 완료되면, LSTM에 따른 가장 가능성이 높은 번역을 찾아서 번역들을 찾아낸다.
$$\hat{T} = argmax_{T} \ p(T|S)$$
- 가장 가능성이 높은 번역을 부분 가설의 작은 수 B를 유지하는 간단한 left-to-right beam search 디코더를 사용한다.
- 부분 가설은 몇몇 번역의 접두어이다.
- 각 시간 단계마다. 빔을 통해 단어장에서 가능한 모든 단어들로 부분 가설을 확장한다.
- 이것이 가설의 수가 크게 증가하여 우리가 모델의 log가능성에 따라 가장 그럴듯한 가설 B를 제외하고 모두 버릴 수 있다.
- \<EOS\>기호가 가설에 추가되자마다 빔에서 지워지고 완성된 가설의 집합에 더해진다.
- 이 디코더가 대력적인 반면에 수행하기 쉽다.
- 흥미롭게도, 이 시스템이 beam 사이즈 1과 2에서도 잘 수행하여 beam serach의 이점을 준다.

- LSTM을 사용해서 STM 베이스라인 시스템의 베이스라인에서 생성된 1,000개의 가장 좋은 목록을 다시 점수를 매겼다.
- 가장 좋았던 목록을 점수를 다시 매기기 위해서, 모든 가설을 LSTM을 가지고 Log 가능성을 연산해서 LSTM과 STM의 평균 점수를 구했다.

### 소스 문장을 뒤집기
- LSTM이 장기 의존성 문제를 해결하는 반면, 
- LSTM은 소스 문장이 뒤집혀졌을 때 더 잘 학습한다는 것을 발견했다.(타겟은 뒤집지 않는다.)
- 그렇게 해서 LSTM의 테스트가 이해하지 못하는 경우가 5.8에서 4.7로 떨어졌고, 디코딩된 번역의 BLEU점수도 25.9에서 30.6으로 올라갔다.
- 이러한 현상을 완전하게 설명하지 못하지만, 이것이 데이터셋의 단기 의존성을 도입하였기 때문이라고 생각한다.
- 보통 소스 문장이 타겟 문장으로 합쳐질 때, 소스의 각 단어들이 타겟문장에서 일치하는 단어들이 멀다.
- 결과적으로 '최소 시간지연'을 키우게 되는 문제가 발생한다.
- 소스 문장을 뒤집어 줄 때, 소스와 타겟 언어의 상응하는 단어의 평균 거리는 바뀌지 않는다.
- 하지만, 소스 언어의 첫 번째 몇몇 단어들이 이제 타겟 언어의 첫 단어들과 아주 가까워 져서 '최소 시간지연' 문제가 크게 줄어든다. 따라서 역전파가 소스 문장과 타겟 문장사이의 '소통 생성'에 더 쉬운 시간을 갖는다.
- 차례로 상당히 향상된 전체 성능을 얻어낸다.
- 

# Material to study
- [[end-to-end learning]]
- 
>>>>>>> 94bb7169 (test blog)
